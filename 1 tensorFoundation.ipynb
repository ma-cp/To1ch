{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# This is a sample Jupyter Notebook\n",
    "\n",
    "Below is an example of a code cell. \n",
    "Put your cursor into the cell and press Shift+Enter to execute it and select the next one, or click 'Run Cell' button.\n",
    "\n",
    "Press Double Shift to search everywhere for classes, files, tool windows, actions, and settings.\n",
    "\n",
    "To learn more about Jupyter Notebooks in PyCharm, see [help](https://www.jetbrains.com/help/pycharm/ipython-notebook-support.html).\n",
    "For an overview of PyCharm, go to Help -> Learn IDE features or refer to [our documentation](https://www.jetbrains.com/help/pycharm/getting-started.html)."
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "上面是Pycharm自动创建的内容，在一本书里看到说“使用IDE的前提是你得知道它替你做了什么工作”\n",
    "\n",
    "带着这个疑虑，我发现自己实际上现在并不完全知道IDE实际上干了什么，\n",
    "\n",
    "但是我知道它做的这些工作没了它我依然能做，此之谓迷之自信，\n",
    "\n",
    "于是接下来的工作便抛下对于效率工具的纠结，而义无反顾地朝着pytorch走去。"
   ],
   "id": "be5d0cbf84fb9907"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T09:04:31.972178Z",
     "start_time": "2025-09-25T09:04:30.779519Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch # Windows平台的torch会依赖一个c++库，第一次运行报错，然后报错内容中跳出一个链接，点那个链接就能下载c++库，不得不再次感叹前人的工作实属伟大。\n",
    "\n",
    "print(\"Hello World!\")\n",
    "print(\"Hello Pytorch!\")"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n",
      "Hello Pytorch!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "pytorch能干什么？最基础的是它作为“张量”的功能，或者说，Pytorch这个工具是面向张量的。\n",
    "\n",
    "说是张量，实际上是高维矩阵的感觉。\n",
    "\n",
    "下面展示了创建张量的方法。\n",
    "\n",
    "## 创建张量"
   ],
   "id": "f9943141f0916e5c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T09:04:31.998606Z",
     "start_time": "2025-09-25T09:04:31.987670Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tensor1 = torch.Tensor([[1,2,3],[4,5,6],[7,8,9]]) # 指定数据 独特\n",
    "torch.rand(3, 4) # 均匀分布3x4的矩阵\n",
    "torch.randn(3, 4) # 正态分布\n",
    "torch.ones(2, 2) # 全1矩阵\n",
    "torch.zeros(5) # 全0矩阵\n",
    "torch.eye(3) # 单位矩阵\n",
    "\n",
    "print(f\"同形状创建0: {torch.ones_like(tensor1)}\") # 特别的like\n",
    "print(f\"同形状创建正态: {torch.randn_like(tensor1)}\")\n",
    "\n",
    "print(f\"序列向量：{torch.arange(0, 10, 2)}\")\n",
    "print(f\"间隔向量：{torch.linspace(0, 1, 5)}\")\n",
    "\n",
    "torch.empty(2, 3) # 空Tensor 速度快\n",
    "\n",
    "torch.rand(3,4,5)"
   ],
   "id": "74c5eda9c5864b1f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "同形状创建0: tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "同形状创建正态: tensor([[-0.3518, -0.2258, -0.9328],\n",
      "        [ 1.1973,  1.1967,  0.2810],\n",
      "        [-0.6118,  0.2083,  1.6679]])\n",
      "序列向量：tensor([0, 2, 4, 6, 8])\n",
      "间隔向量：tensor([0.0000, 0.2500, 0.5000, 0.7500, 1.0000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.6821, 0.9730, 0.4697, 0.1855, 0.7065],\n",
       "         [0.9155, 0.0104, 0.0680, 0.0458, 0.1672],\n",
       "         [0.0433, 0.7736, 0.6876, 0.1876, 0.1612],\n",
       "         [0.7780, 0.0851, 0.6414, 0.5783, 0.4943]],\n",
       "\n",
       "        [[0.4224, 0.5992, 0.4666, 0.9647, 0.9477],\n",
       "         [0.1516, 0.1021, 0.7142, 0.3251, 0.5272],\n",
       "         [0.8428, 0.8626, 0.9882, 0.9835, 0.0380],\n",
       "         [0.3724, 0.7892, 0.4767, 0.1280, 0.6878]],\n",
       "\n",
       "        [[0.6057, 0.8016, 0.5999, 0.6201, 0.1668],\n",
       "         [0.0404, 0.4640, 0.8885, 0.4860, 0.8520],\n",
       "         [0.3153, 0.5924, 0.7995, 0.1664, 0.4602],\n",
       "         [0.4169, 0.8026, 0.9359, 0.4172, 0.9851]]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "上方的代码块展示了如何创建各种形状的张量，可以创建确定的张量，也可以创建一些快捷张量\n",
    "\n",
    "学会了如何创建张量之后，就会有迫切的欲望对张量进行一些操作，增删改查的想法在这里是合适的。\n",
    "\n",
    "说是增删改查，研究对象可以是对整个表的结构，也可以是表中的值。\n",
    "\n",
    "*首先我们从结构上来操作整张表，即，暂时只关心“维度”的问题*\n",
    "\n",
    "增，就认为是增加维度。\n",
    "\n",
    "删，就认为是删除维度。\n",
    "\n",
    "改，就认为是更改某个维度的值。\n",
    "\n",
    "查，查看张量的维度。\n",
    "\n",
    "下面是对张量进行维度层面的增删改查的例子。\n",
    "\n",
    "## 维度操作"
   ],
   "id": "8ac01a77ca8364f8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T09:04:32.008458Z",
     "start_time": "2025-09-25T09:04:32.002440Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tensor2 = torch.randn(2, 3)\n",
    "print(tensor2)\n",
    "print(\"查\")\n",
    "print(tensor2.shape) # 查看tensor的形状\n",
    "print(tensor2 > 0) # 布尔掩码\n",
    "print(\"增\")\n",
    "print(tensor2.unsqueeze(1).shape) # 指定位置扩展一个空维度\n",
    "print(\"删\")\n",
    "print(tensor2.squeeze().shape) # 删除所有空维度（思考：为什么只能删除空维度？）\n",
    "print(\"改\")\n",
    "print(tensor2[1, :].shape) # 从某个维度切片（想删某个维度，实际上就是把想留下的部分切片出来）\n",
    "tensor2_1 = torch.randn(2, 4) # 一个张量维度扩展后，只有和其它张量拼在一起才能体现扩展的作用。\n",
    "tensor2_2 = torch.randn(1, 3)\n",
    "print(torch.cat([tensor2, tensor2_1], dim = 1).shape) # 保持其他维度不变，将第1维度的3和4拼起来\n",
    "print(torch.cat([tensor2, tensor2_2], dim = 0).shape) # 将第0维度的2和1拼起来\n",
    "print(tensor2.reshape(6, 1).shape) # 重新设置维度，注意元素个数保持不变\n",
    "print(tensor2.permute(1, 0).shape) # 维度互换"
   ],
   "id": "1c912903744da5e9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7111,  0.0178,  0.2009],\n",
      "        [ 0.4159,  0.9083, -0.2205]])\n",
      "查\n",
      "torch.Size([2, 3])\n",
      "tensor([[False,  True,  True],\n",
      "        [ True,  True, False]])\n",
      "增\n",
      "torch.Size([2, 1, 3])\n",
      "删\n",
      "torch.Size([2, 3])\n",
      "改\n",
      "torch.Size([3])\n",
      "torch.Size([2, 7])\n",
      "torch.Size([3, 3])\n",
      "torch.Size([6, 1])\n",
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "经过上面的维度体操，可以说已经掌握了对张量在宏观角度的操作。\n",
    "\n",
    "在微观上，也就是说对张量中的值，也需要进行一些操作，但由于维度确定了，值的存在性也就确定了，对值的增删就没有了意义，所以只需要关注对值的“改、查”两个操作。\n",
    "\n",
    "## 数值操作"
   ],
   "id": "be61e6844c926797"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T09:04:32.023431Z",
     "start_time": "2025-09-25T09:04:32.012948Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tensor3 = torch.randn(2, 3, 4)\n",
    "print(tensor3)\n",
    "print(\"查\")\n",
    "print(tensor3.dtype) # tensor的元素的类型\n",
    "print(tensor3[1, :].shape) # 从某个维度切片（切片这个行为，可以看作是高纬度的改，也可以看作是低维度的查）\n",
    "print(tensor3[:,2].shape) # 从某个维度切片\n",
    "print(tensor3>0) # 布尔掩码\n",
    "print(tensor3[tensor3 > 0]) # 布尔掩码切片，会返回一个一维序列\n",
    "print(tensor3[0,1][2]) # 直接索引查找是很灵活的\n",
    "print(tensor3[0,1,2])\n",
    "print(tensor3[[0,1],[0,2],:]) #相当于[0,0,:]和[1,2,:]\n",
    "print(\"查统计数据\")\n",
    "print(\"平均数\")\n",
    "print(tensor3.mean())\n",
    "print(\"中位数\")\n",
    "print(tensor3.median())\n",
    "print(\"标准差\")\n",
    "print(tensor3.std())\n",
    "print(\"最值\")\n",
    "print(tensor3.max())\n",
    "print(tensor3.min())\n",
    "\n",
    "print(\"改\")\n",
    "print(\"直接加法\")\n",
    "print(tensor3+1)\n",
    "print(\"乘法\")\n",
    "print(tensor3.mul(10))\n",
    "print(\"下面这个依然是数字乘法而不是矩阵乘法，但是对每个元素乘的权重不同，可以叫它权重乘法，会实现两个张量中相同位置上的值的运算\")\n",
    "print(tensor3.mul(tensor3))\n",
    "print(\"原地操作\")\n",
    "print(tensor3.mul_(10))\n",
    "print(\"张量乘法\")\n",
    "print(\"高维张量乘法的规则是，最后两个维度（最内部）需要符合矩阵乘法的规则(内标相同)，而其它维度需要相等或者可广播\") # 可广播指的是维度为1\n",
    "tensor3_1 = torch.randn(2,4,2).mul(10)\n",
    "print(\"（同时伴随着一个类型转换操作）\")\n",
    "print((tensor3@(tensor3_1)).to(torch.int))\n",
    "\n"
   ],
   "id": "f0652b5125429336",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.9788, -0.6459, -1.1626, -0.9730],\n",
      "         [ 0.4682, -1.0621, -0.9870, -1.5242],\n",
      "         [-0.2596,  0.5855, -1.7444,  0.7711]],\n",
      "\n",
      "        [[ 1.2049, -0.7256, -0.6298, -0.0086],\n",
      "         [ 1.6113, -0.6415, -1.0569, -2.9428],\n",
      "         [ 0.5425, -1.6116,  1.2211, -0.7645]]])\n",
      "查\n",
      "torch.float32\n",
      "torch.Size([3, 4])\n",
      "torch.Size([2, 4])\n",
      "tensor([[[False, False, False, False],\n",
      "         [ True, False, False, False],\n",
      "         [False,  True, False,  True]],\n",
      "\n",
      "        [[ True, False, False, False],\n",
      "         [ True, False, False, False],\n",
      "         [ True, False,  True, False]]])\n",
      "tensor([0.4682, 0.5855, 0.7711, 1.2049, 1.6113, 0.5425, 1.2211])\n",
      "tensor(-0.9870)\n",
      "tensor(-0.9870)\n",
      "tensor([[-1.9788, -0.6459, -1.1626, -0.9730],\n",
      "        [ 0.5425, -1.6116,  1.2211, -0.7645]])\n",
      "查统计数据\n",
      "平均数\n",
      "tensor(-0.5131)\n",
      "中位数\n",
      "tensor(-0.7256)\n",
      "标准差\n",
      "tensor(1.1254)\n",
      "最值\n",
      "tensor(1.6113)\n",
      "tensor(-2.9428)\n",
      "改\n",
      "直接加法\n",
      "tensor([[[-0.9788,  0.3541, -0.1626,  0.0270],\n",
      "         [ 1.4682, -0.0621,  0.0130, -0.5242],\n",
      "         [ 0.7404,  1.5855, -0.7444,  1.7711]],\n",
      "\n",
      "        [[ 2.2049,  0.2744,  0.3702,  0.9914],\n",
      "         [ 2.6113,  0.3585, -0.0569, -1.9428],\n",
      "         [ 1.5425, -0.6116,  2.2211,  0.2355]]])\n",
      "乘法\n",
      "tensor([[[-19.7881,  -6.4587, -11.6257,  -9.7301],\n",
      "         [  4.6821, -10.6209,  -9.8697, -15.2425],\n",
      "         [ -2.5962,   5.8553, -17.4440,   7.7109]],\n",
      "\n",
      "        [[ 12.0492,  -7.2560,  -6.2979,  -0.0862],\n",
      "         [ 16.1128,  -6.4146, -10.5693, -29.4277],\n",
      "         [  5.4246, -16.1157,  12.2106,  -7.6447]]])\n",
      "下面这个依然是数字乘法而不是矩阵乘法，但是对每个元素乘的权重不同，可以叫它权重乘法，会实现两个张量中相同位置上的值的运算\n",
      "tensor([[[3.9157e+00, 4.1715e-01, 1.3516e+00, 9.4675e-01],\n",
      "         [2.1922e-01, 1.1280e+00, 9.7411e-01, 2.3233e+00],\n",
      "         [6.7404e-02, 3.4284e-01, 3.0429e+00, 5.9458e-01]],\n",
      "\n",
      "        [[1.4518e+00, 5.2650e-01, 3.9663e-01, 7.4336e-05],\n",
      "         [2.5962e+00, 4.1147e-01, 1.1171e+00, 8.6599e+00],\n",
      "         [2.9426e-01, 2.5972e+00, 1.4910e+00, 5.8442e-01]]])\n",
      "原地操作\n",
      "tensor([[[-19.7881,  -6.4587, -11.6257,  -9.7301],\n",
      "         [  4.6821, -10.6209,  -9.8697, -15.2425],\n",
      "         [ -2.5962,   5.8553, -17.4440,   7.7109]],\n",
      "\n",
      "        [[ 12.0492,  -7.2560,  -6.2979,  -0.0862],\n",
      "         [ 16.1128,  -6.4146, -10.5693, -29.4277],\n",
      "         [  5.4246, -16.1157,  12.2106,  -7.6447]]])\n",
      "张量乘法\n",
      "高维张量乘法的规则是，最后两个维度（最内部）需要符合矩阵乘法的规则(内标相同)，而其它维度需要相等或者可广播\n",
      "（同时伴随着一个类型转换操作）\n",
      "tensor([[[ 273, -198],\n",
      "         [ 351, -576],\n",
      "         [ 500,  126]],\n",
      "\n",
      "        [[-173,  -41],\n",
      "         [-531,  103],\n",
      "         [-252,   -1]]], dtype=torch.int32)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "除了上面的“创建、操作维度、操作元素”，还有一个值得关注的方面是张量与硬件的关系。\n",
    "\n",
    "张量如何保存到硬盘中？如何从硬盘中取出？\n",
    "\n",
    "张量如何运行在CPU中或是GPU中？\n",
    "\n",
    "## 硬件相关"
   ],
   "id": "a2e7f918ed2cc530"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T09:04:32.031751Z",
     "start_time": "2025-09-25T09:04:32.027437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tensor4 = torch.empty(2, 3)\n",
    "print(tensor4.device) # 当前设备，CPU或者GPU\n",
    "tensor4.to(\"cpu\") # 改变设备，和类型转换是一个语法。\n",
    "\n",
    "print(tensor4.requires_grad) # 查看自动求导状态，默认是关着的\n",
    "print(tensor4.requires_grad_(True)) # 设置自动求导状态\n",
    "print(tensor4.requires_grad_(False))\n",
    "print(tensor4.requires_grad)"
   ],
   "id": "87b7dccaf8b9635b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "False\n",
      "tensor([[2.3694e-38, 2.3694e-38, 2.3694e-38],\n",
      "        [2.3694e-38, 2.3694e-38, 2.3694e-38]], requires_grad=True)\n",
      "tensor([[2.3694e-38, 2.3694e-38, 2.3694e-38],\n",
      "        [2.3694e-38, 2.3694e-38, 2.3694e-38]])\n",
      "False\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "torch有它自己的特色，专门有利于AI领域的特色：对反向传播的支持\n",
    "## 反向传播"
   ],
   "id": "ff2b3a21a0d68381"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T09:04:32.044364Z",
     "start_time": "2025-09-25T09:04:32.038582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tensor5=torch.arange(0,6).reshape(2,3).to(torch.float)\n",
    "print(\"原数据\")\n",
    "print(tensor5)\n",
    "print(\"打开自动求导：\")\n",
    "tensor5.requires_grad_(True)\n",
    "print(tensor5)\n",
    "tensor5m=torch.mean(tensor5.mul(tensor5))\n",
    "print(f\"求元素平方的平均值{tensor5m}\")\n",
    "print(\"反向传播前的梯度\")\n",
    "print(tensor5.grad)\n",
    "tensor5m.backward()\n",
    "print(\"反向传播后的梯度\")\n",
    "print(tensor5.grad)\n",
    "print(\"梯度变了不会自动影响原数据\")\n",
    "print(tensor5)"
   ],
   "id": "2f22e3def529ad49",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原数据\n",
      "tensor([[0., 1., 2.],\n",
      "        [3., 4., 5.]])\n",
      "打开自动求导：\n",
      "tensor([[0., 1., 2.],\n",
      "        [3., 4., 5.]], requires_grad=True)\n",
      "求元素平方的平均值9.166666984558105\n",
      "反向传播前的梯度\n",
      "None\n",
      "反向传播后的梯度\n",
      "tensor([[0.0000, 0.3333, 0.6667],\n",
      "        [1.0000, 1.3333, 1.6667]])\n",
      "梯度变了不会自动影响原数据\n",
      "tensor([[0., 1., 2.],\n",
      "        [3., 4., 5.]], requires_grad=True)\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
